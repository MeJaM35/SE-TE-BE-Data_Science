{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeMxzbM4O3tb",
        "outputId": "a967334b-ec93-49f0-e41a-2f815256fe8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster labels: [0 0 1 1 0]\n",
            "\n",
            "Cluster 0:\n",
            "  The car is fast and new.\n",
            "  I love fast cars.\n",
            "  Cars are great for fast transportation.\n",
            "\n",
            "Cluster 1:\n",
            "  The sun is bright and shining.\n",
            "  I love the warm weather.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Sample text documents\n",
        "documents = [\n",
        "    \"The car is fast and new.\",\n",
        "    \"I love fast cars.\",\n",
        "    \"The sun is bright and shining.\",\n",
        "    \"I love the warm weather.\",\n",
        "    \"Cars are great for fast transportation.\"\n",
        "]\n",
        "\n",
        "# Step 1: TF-IDF Vectorization (using words)\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Apply KMeans clustering\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Output the cluster labels\n",
        "print(\"Cluster labels:\", kmeans.labels_)\n",
        "\n",
        "# Print which documents belong to which clusters\n",
        "for cluster_id in np.unique(kmeans.labels_):\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    cluster_docs = [documents[i] for i in range(len(documents)) if kmeans.labels_[i] == cluster_id]\n",
        "    for doc in cluster_docs:\n",
        "        print(f\"  {doc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize stopwords, stemmer, and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "punctuation_table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The team scored a goal in the game.\",\n",
        "    \"Election results show a victory for the candidate.\",\n",
        "    \"The player scored the winning goal.\",\n",
        "    \"Climate change is affecting the environment.\",\n",
        "    \"The vote count favored the candidate.\"\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    text = text.translate(punctuation_table)  # Remove punctuation\n",
        "    tokens = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "# Preprocess documents\n",
        "documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Step 1: Extract Word Clusters using TF-IDF and KMeans\n",
        "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1, 1))\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Perform clustering on word vectors\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(tfidf_matrix)\n",
        "\n",
        "# Assign clusters to documents\n",
        "doc_clusters = kmeans.predict(tfidf_matrix)\n",
        "\n",
        "# Print document clusters\n",
        "print(\"Word-Based Clusters:\")\n",
        "for i, cluster in enumerate(doc_clusters):\n",
        "    print(f\"Document {i + 1}: Cluster {cluster}\")\n",
        "\n",
        "# Step 2: Extract Frequent Phrases using Spacy\n",
        "def extract_phrases_spacy(documents, n=2):\n",
        "    phrase_counter = Counter()\n",
        "    for doc in documents:\n",
        "        tokens = [token.text for token in nlp(doc)]\n",
        "        phrases = list(ngrams(tokens, n))\n",
        "        phrase_counter.update(phrases)\n",
        "    return phrase_counter\n",
        "\n",
        "# Extract bigrams (phrases of length 2)\n",
        "phrase_counts = extract_phrases_spacy(documents, n=2)\n",
        "\n",
        "# Filter top phrases\n",
        "top_phrases = phrase_counts.most_common(5)\n",
        "print(\"\\nTop Phrases:\")\n",
        "for phrase, count in top_phrases:\n",
        "    print(f\"{' '.join(phrase)}: {count}\")\n",
        "\n",
        "# Step 3: Assign documents to phrase-based clusters\n",
        "phrase_clusters = {}\n",
        "for phrase, _ in top_phrases:\n",
        "    phrase_clusters[' '.join(phrase)] = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        if ' '.join(phrase) in doc:\n",
        "            phrase_clusters[' '.join(phrase)].append(f\"Document {i + 1}\")\n",
        "\n",
        "print(\"\\nPhrase-Based Clusters:\")\n",
        "for phrase, docs in phrase_clusters.items():\n",
        "    print(f\"Phrase '{phrase}': {', '.join(docs)}\")\n"
      ],
      "metadata": {
        "id": "J0GbddudbbyC",
        "outputId": "f195bdc1-a840-4bf0-c281-1fd12f3937ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word-Based Clusters:\n",
            "Document 1: Cluster 2\n",
            "Document 2: Cluster 0\n",
            "Document 3: Cluster 2\n",
            "Document 4: Cluster 1\n",
            "Document 5: Cluster 0\n",
            "\n",
            "Top Phrases:\n",
            "team score: 1\n",
            "score goal: 1\n",
            "goal game: 1\n",
            "elect result: 1\n",
            "result show: 1\n",
            "\n",
            "Phrase-Based Clusters:\n",
            "Phrase 'team score': Document 1\n",
            "Phrase 'score goal': Document 1\n",
            "Phrase 'goal game': Document 1\n",
            "Phrase 'elect result': Document 2\n",
            "Phrase 'result show': Document 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The team scored a goal in the game.\",\n",
        "    \"Election results show a victory for the candidate.\",\n",
        "    \"The player scored the winning goal.\",\n",
        "    \"Climate change is affecting the environment.\",\n",
        "    \"The vote count favored the candidate.\"\n",
        "]\n",
        "\n",
        "# Manually defined stopwords (basic list)\n",
        "stopwords = [\n",
        "    'the', 'a', 'and', 'for', 'of', 'to', 'is', 'in', 'on', 'with', 'at', 'by', 'as', 'this', 'that', 'from'\n",
        "]\n",
        "\n",
        "# Step 1: Extract Word Clusters (Co-occurrence Analysis)\n",
        "\n",
        "# Manually defined word clusters (for simplicity)\n",
        "word_clusters = {\n",
        "    \"Sports\": {\"team\", \"goal\", \"game\", \"player\"},\n",
        "    \"Politics\": {\"election\", \"vote\", \"candidate\"},\n",
        "    \"Environment\": {\"climate\", \"change\", \"environment\"}\n",
        "}\n",
        "\n",
        "# Step 2: Represent Documents Using Word Clusters\n",
        "\n",
        "def represent_documents(documents, word_clusters):\n",
        "    doc_representation = []\n",
        "    for doc in documents:\n",
        "        doc_words = set(doc.lower().split())  # tokenize and make lower case\n",
        "        doc_cluster = set()\n",
        "        for cluster_name, words in word_clusters.items():\n",
        "            if not doc_words.isdisjoint(words):\n",
        "                doc_cluster.add(cluster_name)\n",
        "        doc_representation.append(doc_cluster)\n",
        "    return doc_representation\n",
        "\n",
        "doc_representation = represent_documents(documents, word_clusters)\n",
        "\n",
        "# Display the document representation based on word clusters\n",
        "for i, doc in enumerate(doc_representation):\n",
        "    print(f\"Document {i+1}: {doc}\")\n",
        "\n",
        "# Step 3: Cluster Documents Based on Word Clusters\n",
        "\n",
        "# Convert the set of clusters into a binary vector\n",
        "def cluster_vector(doc_cluster, all_clusters):\n",
        "    return [1 if cluster in doc_cluster else 0 for cluster in all_clusters]\n",
        "\n",
        "all_clusters = list(word_clusters.keys())\n",
        "doc_vectors = [cluster_vector(doc, all_clusters) for doc in doc_representation]\n",
        "\n",
        "# Perform KMeans clustering\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=0).fit(doc_vectors)\n",
        "\n",
        "# Show clustering results\n",
        "print(\"\\nDocument Clusters (based on word clusters):\")\n",
        "for i, label in enumerate(kmeans.labels_):\n",
        "    print(f\"Document {i+1} is in Cluster {label}\")\n",
        "\n",
        "# Step 4: Phrase-Based Clustering (Frequent Phrase Mining)\n",
        "\n",
        "# Define a function for extracting frequent phrases (bigrams) using TF-IDF\n",
        "def extract_frequent_phrases(documents, n=2, top_n=5):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(n, n), stop_words=stopwords)\n",
        "    X = vectorizer.fit_transform(documents)\n",
        "    tfidf_scores = np.asarray(X.sum(axis=0)).flatten()\n",
        "    phrases = vectorizer.get_feature_names_out()\n",
        "    phrase_score_dict = dict(zip(phrases, tfidf_scores))\n",
        "\n",
        "    # Sort by TF-IDF score and get top N phrases\n",
        "    return sorted(phrase_score_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# Extract frequent 2-grams (bigrams) from the documents using TF-IDF\n",
        "frequent_phrases = extract_frequent_phrases(documents)\n",
        "\n",
        "# Display the most frequent phrases\n",
        "print(\"\\nFrequent Phrases (using TF-IDF):\")\n",
        "for phrase, score in frequent_phrases:\n",
        "    print(f\"'{phrase}': {score}\")\n",
        "\n",
        "# Step 5: Cluster Documents Based on Phrases\n",
        "# Use a simple approach where we classify documents based on the presence of the frequent phrases\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWuTar70HGb0",
        "outputId": "aeec393b-eeec-4f78-e966-ce12ae42fdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: {'Sports'}\n",
            "Document 2: {'Politics'}\n",
            "Document 3: {'Sports'}\n",
            "Document 4: {'Environment'}\n",
            "Document 5: {'Politics'}\n",
            "\n",
            "Document Clusters (based on word clusters):\n",
            "Document 1 is in Cluster 0\n",
            "Document 2 is in Cluster 1\n",
            "Document 3 is in Cluster 0\n",
            "Document 4 is in Cluster 2\n",
            "Document 5 is in Cluster 1\n",
            "\n",
            "Frequent Phrases (using TF-IDF):\n",
            "'affecting environment': 0.5773502691896258\n",
            "'change affecting': 0.5773502691896258\n",
            "'climate change': 0.5773502691896258\n",
            "'count favored': 0.5773502691896258\n",
            "'favored candidate': 0.5773502691896258\n",
            "\n",
            "Phrase-Based Clustering Results:\n"
          ]
        }
      ]
    }
  ]
}